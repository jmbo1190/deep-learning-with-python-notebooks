---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.0
  kernelspec:
    display_name: TF2.6Py3.7
    language: python
    name: tf2.6py3.7
---

<!-- #region colab_type="text" -->
This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.

**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**

This notebook was generated for TensorFlow 2.6.
<!-- #endregion -->

```{python}
# Get current python environment - depends on selected kernel
import os
import sys
print("os.getcwd(): " + os.getcwd())
print("sys.prefix: ", sys.prefix)
print("sys.exec_prefix: ", sys.exec_prefix)
print("sys.executable: ", sys.executable)
print("os.path.basename(sys.exec_prefix)", os.path.basename(sys.exec_prefix))

import tensorflow as tf
print(f"TensorFlow Version: {tf.__version__}")

if tf.__version__ >= "2.":
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    tf.config.list_physical_devices('GPU')
    import tensorflow.keras
    print(f"Keras Version: {tensorflow.keras.__version__}")
    gpu = len(tf.config.list_physical_devices('GPU'))>0
    print("GPU is", "available" if gpu else "NOT AVAILABLE")
    if gpu:
        print(tf.config.list_physical_devices('GPU'))
        
```

```{python}
# Get installed package versions in virtual environment associated to Jupyter kernel
import pkg_resources

pkgs = dict([(d.__dict__.get("_key"), d.__dict__.get("_version")) for d in pkg_resources.working_set])

for k in sorted(pkgs.keys()):
    print(k, pkgs.get(k))
```

```{python}
import subprocess
import sys
import pkg_resources


# Calling [sys.executable, '-m', 'pip', 'install', name] 
# rather than ['pip', 'install', name]
# is making sure to get the "right" pip (i.e. you install in current virtual environment)
def install(package):
    # Get installed package versions in virtual environment associated to Jupyter kernel
    pkgs = dict([(d.__dict__.get("_key"), d.__dict__.get("_version")) for d in pkg_resources.working_set])
    if package not in pkgs.keys():
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    else:
        print(f"Package {package} version {pkgs.get(package)} is already installed")
    
install("ipython-autotime")
# %load_ext autotime
```

<!-- #region colab_type="text" -->
## Interpreting what convnets learn
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Visualizing intermediate activations
<!-- #endregion -->

```{python colab_type="code"}
# You can use this to load the file "convnet_from_scratch_with_augmentation.keras"
# you obtained in the last chapter.
# from google.colab import files
# files.upload()
```

```{python colab_type="code"}
from tensorflow import keras
from GAmodel import GAModel

#model = keras.models.load_model("convnet_from_scratch_with_augmentation.keras")
model = keras.models.load_model("convnet_from_scratch_with_augmentation.tf")
```

```{python colab_type="code"}
model.summary()
```

<!-- #region colab_type="text" -->
**Preprocessing a single image**
<!-- #endregion -->

```{python colab_type="code"}
from tensorflow import keras
import numpy as np

img_path = keras.utils.get_file(
    fname="cat.jpg",
    origin="https://img-datasets.s3.amazonaws.com/cat.jpg")

def get_img_array(img_path, target_size):
    img = keras.utils.load_img(
        img_path, target_size=target_size)
    array = keras.utils.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    return array

img_tensor = get_img_array(img_path, target_size=(180, 180))
```

<!-- #region colab_type="text" -->
**Displaying the test picture**
<!-- #endregion -->

```{python colab_type="code"}
import matplotlib.pyplot as plt
plt.axis("off")
plt.imshow(img_tensor[0].astype("uint8"))
plt.show()
```

<!-- #region colab_type="text" -->
**Instantiating a model that returns layer activations**
<!-- #endregion -->

```{python colab_type="code"}
from tensorflow.keras import layers

layer_outputs = []
layer_names = []
for layer in model.layers:
    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):
        layer_outputs.append(layer.output)
        layer_names.append(layer.name)
activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)
```

<!-- #region colab_type="text" -->
**Using the model to compute layer activations**
<!-- #endregion -->

```{python colab_type="code"}
activations = activation_model.predict(img_tensor)
```

```{python colab_type="code"}
first_layer_activation = activations[0]
print(first_layer_activation.shape)
```

<!-- #region colab_type="text" -->
**Visualizing the fifth channel**
<!-- #endregion -->

```{python colab_type="code"}
import matplotlib.pyplot as plt
plt.matshow(first_layer_activation[0, :, :, 5], cmap="viridis")
```

<!-- #region colab_type="text" -->
**Visualizing every channel in every intermediate activation**
<!-- #endregion -->

```{python colab_type="code"}
images_per_row = 16
for layer_name, layer_activation in zip(layer_names, activations):
    n_features = layer_activation.shape[-1]
    size = layer_activation.shape[1]
    n_cols = n_features // images_per_row
    display_grid = np.zeros(((size + 1) * n_cols - 1,
                             images_per_row * (size + 1) - 1))
    for col in range(n_cols):
        for row in range(images_per_row):
            channel_index = col * images_per_row + row
            channel_image = layer_activation[0, :, :, channel_index].copy()
            if channel_image.sum() != 0:
                channel_image -= channel_image.mean()
                channel_image /= channel_image.std()
                channel_image *= 64
                channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype("uint8")
            display_grid[
                col * (size + 1): (col + 1) * size + col,
                row * (size + 1) : (row + 1) * size + row] = channel_image
    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.axis("off")
    plt.imshow(display_grid, aspect="auto", cmap="viridis")
```

<!-- #region colab_type="text" -->
### Visualizing convnet filters
<!-- #endregion -->

<!-- #region colab_type="text" -->
**Instantiating the Xception convolutional base**
<!-- #endregion -->

```{python colab_type="code"}
model = keras.applications.xception.Xception(
    weights="imagenet",
    include_top=False)
```

<!-- #region colab_type="text" -->
**Printing the names of all convolutional layers in Xception**
<!-- #endregion -->

```{python colab_type="code"}
for layer in model.layers:
    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):
        print(layer.name)
```

<!-- #region colab_type="text" -->
**Creating a feature extractor model**
<!-- #endregion -->

```{python colab_type="code"}
layer_name = "block3_sepconv1"
layer = model.get_layer(name=layer_name)
feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)
```

<!-- #region colab_type="text" -->
**Using the feature extractor**
<!-- #endregion -->

```{python colab_type="code"}
activation = feature_extractor(
    keras.applications.xception.preprocess_input(img_tensor)
)
```

```{python colab_type="code"}
import tensorflow as tf

def compute_loss(image, filter_index):
    activation = feature_extractor(image)
    filter_activation = activation[:, 2:-2, 2:-2, filter_index]
    return tf.reduce_mean(filter_activation)
```

<!-- #region colab_type="text" -->
**Loss maximization via stochastic gradient ascent**
<!-- #endregion -->

```{python colab_type="code"}
@tf.function
def gradient_ascent_step(image, filter_index, learning_rate):
    with tf.GradientTape() as tape:
        tape.watch(image)
        loss = compute_loss(image, filter_index)
    grads = tape.gradient(loss, image)
    grads = tf.math.l2_normalize(grads)
    image += learning_rate * grads
    return image
```

<!-- #region colab_type="text" -->
**Function to generate filter visualizations**
<!-- #endregion -->

```{python colab_type="code"}
img_width = 200
img_height = 200

def generate_filter_pattern(filter_index):
    iterations = 30
    learning_rate = 10.
    image = tf.random.uniform(
        minval=0.4,
        maxval=0.6,
        shape=(1, img_width, img_height, 3))
    for i in range(iterations):
        image = gradient_ascent_step(image, filter_index, learning_rate)
    return image[0].numpy()
```

<!-- #region colab_type="text" -->
**Utility function to convert a tensor into a valid image**
<!-- #endregion -->

```{python colab_type="code"}
def deprocess_image(image):
    image -= image.mean()
    image /= image.std()
    image *= 64
    image += 128
    image = np.clip(image, 0, 255).astype("uint8")
    image = image[25:-25, 25:-25, :]
    return image
```

```{python colab_type="code"}
plt.axis("off")
plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))
```

<!-- #region colab_type="text" -->
**Generating a grid of all filter response patterns in a layer**
<!-- #endregion -->

```{python colab_type="code"}
all_images = []
for filter_index in range(64):
    print(f"Processing filter {filter_index}")
    image = deprocess_image(
        generate_filter_pattern(filter_index)
    )
    all_images.append(image)

margin = 5
n = 8
cropped_width = img_width - 25 * 2
cropped_height = img_height - 25 * 2
width = n * cropped_width + (n - 1) * margin
height = n * cropped_height + (n - 1) * margin
stitched_filters = np.zeros((width, height, 3))

for i in range(n):
    for j in range(n):
        image = all_images[i * n + j]
        stitched_filters[
            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,
            (cropped_height + margin) * j : (cropped_height + margin) * j
            + cropped_height,
            :,
        ] = image

keras.utils.save_img(
    f"filters_for_layer_{layer_name}.png", stitched_filters)
print(f"filters_for_layer_{layer_name}.png")
```

![filters for layer predictions](filters_for_layer_predictions.png "filters_for_layer_predictions.png")

<!-- #region colab_type="text" -->
### 9.4.3 Visualizing heatmaps of class activation

This general category of techniques is called class activation map (CAM) visualization, and it consists of producing heatmaps of class activation over input images. A class activation heatmap is a 2D grid of scores associated with a specific output class, computed for every location in any input image, indicating how important each location is with respect to the class under consideration.

he specific implementation we’ll use is the one described in an article titled [“Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.”](https://arxiv.org/abs/1610.02391)

Grad-CAM consists of taking the output feature map of a convolution layer, given an input image, and weighing every channel in that feature map by the gradient of
the class with respect to the channel. 
<!-- #endregion -->

<!-- #region colab_type="text" -->
**-- Listing 9.20 Loading the Xception network with pretrained weights --**
<!-- #endregion -->

```{python colab_type="code"}
from tensorflow import keras

# Note that we include the densely connected classifier on top; in all previous cases, we discarded it.
model = keras.applications.xception.Xception(weights="imagenet")
```

<!-- #region colab_type="text" -->
**-- Listing 9.21 Preprocessing an input image for Xception --**
<!-- #endregion -->

```{python colab_type="code"}
import numpy as np

# Download the image and store it locally under the path img_path
img_path = keras.utils.get_file(
    fname="elephant.jpg",
    origin="https://img-datasets.s3.amazonaws.com/elephant.jpg")

print("Downloaded image was stored to:", img_path)

# make a link to downloaded image in current directory
! [ ! -e elephant.jpg ] && ln -s {img_path} elephant.jpg 

def get_img_array(img_path, target_size):
    img = keras.utils.load_img(img_path, target_size=target_size)  # Return a Python Imaging Library (PIL) image of size 299 × 299
    array = keras.utils.img_to_array(img)                          # Return a float32 NumPy array of shape (299, 299, 3)
    array = np.expand_dims(array, axis=0)                          # Add a dimension to transform the array into a batch of size (1, 299, 299, 3)
    array = keras.applications.xception.preprocess_input(array)    # Preprocess the batch (this does channel-wise color normalization).
    return array

img_array = get_img_array(img_path, target_size=(299, 299))

```

```{python}
from IPython.display import display, Markdown #, Latex

display(Markdown(f'![original picture](elephant.jpg "Figure 9.18 Test picture of African elephants")'))

```

```{python colab_type="code"}
# Run the pretrained network on the image and decode its prediction vector back to a human-readable format
preds = model.predict(img_array)
print(keras.applications.xception.decode_predictions(preds, top=3)[0])
```

The entry in the prediction vector that was maximally activated is the one corresponding to the “African elephant” class, at index 386.

```{python colab_type="code"}
np.argmax(preds[0])
```

<!-- #region colab_type="text" -->
First, we create a model that maps the input image to the activations of the last convolutional layer.

**-- Listing 9.22 Setting up a model that returns the last convolutional output --**
<!-- #endregion -->

```{python colab_type="code"}
last_conv_layer_name = "block14_sepconv2_act"
classifier_layer_names = [
    "avg_pool",
    "predictions",
]
last_conv_layer = model.get_layer(last_conv_layer_name)
last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)
```

<!-- #region colab_type="text" -->
Second, we create a model that maps the activations of the last convolutional layer to the final class predictions.

**-- Listing 9.23 Reapplying the classifier on top of the last convolutional output --**
<!-- #endregion -->

```{python colab_type="code"}
classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])
x = classifier_input
for layer_name in classifier_layer_names:
    x = model.get_layer(layer_name)(x)
    
classifier_model = keras.Model(classifier_input, x)

classifier_model.summary()
```

<!-- #region colab_type="text" -->
Then we compute the gradient of the top predicted class for our input image with respect to the activations of the last convolution layer.

**-- Listing 9.24 Retrieving the gradients of the top predicted class --**
<!-- #endregion -->

```{python colab_type="code"}
import tensorflow as tf

with tf.GradientTape() as tape:
    last_conv_layer_output = last_conv_layer_model(img_array)   # Compute activations of the last conv layer and make the tape watch it
    tape.watch(last_conv_layer_output)
    
    # Retrieve the activation channel corresponding to the top predicted class.
    preds = classifier_model(last_conv_layer_output)
    top_pred_index = tf.argmax(preds[0])
    top_class_channel = preds[:, top_pred_index]

# This is the gradient of the top predicted class with regard to the output feature map of the last convolutional layer
grads = tape.gradient(top_class_channel, last_conv_layer_output)

```

<!-- #region colab_type="text" -->
Now we apply pooling and importance weighting to the gradient tensor to obtain our heatmap of class activation.

**-- Listing 9.25 Gradient pooling and channel-importance weighting --**
<!-- #endregion -->

```{python colab_type="code"}
pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()    # This is a vector where each entry is the mean intensity of the
                                                                # gradient for a given channel. It quantifies the importance of
                                                                # each channel with regard to the top predicted class
last_conv_layer_output = last_conv_layer_output.numpy()[0]

for i in range(pooled_grads.shape[-1]):
    last_conv_layer_output[:, :, i] *= pooled_grads[i]          # Multiply each channel in the output of the last convolutional layer 
                                                                # by “how important this channel is.”
        
heatmap = np.mean(last_conv_layer_output, axis=-1)              # The channel-wise mean of the resulting feature
                                                                # map is our heatmap of class activation.
```

<!-- #region colab_type="text" -->
**-- Listing 9.26 Heatmap post-processing --**
<!-- #endregion -->

```{python colab_type="code"}
# For visualization purposes, we normalize the heatmap between 0 and 1
heatmap = np.maximum(heatmap, 0)
heatmap /= np.max(heatmap)

import matplotlib.pyplot as plt

plt.matshow(heatmap)
```

<!-- #region colab_type="text" -->
**-- Listing 9.27 Superimposing the heatmap on the original picture --**
<!-- #endregion -->

```{python colab_type="code"}
import matplotlib.cm as cm

# Load the original image
img = keras.utils.load_img(img_path)
img = keras.utils.img_to_array(img)

heatmap = np.uint8(255 * heatmap)  # Rescale the heatmap to the range 0–255

# Use the "jet" colormap to recolorize the heatmap
jet = cm.get_cmap("jet")
jet_colors = jet(np.arange(256))[:, :3]
jet_heatmap = jet_colors[heatmap]

# Create an image that contains the recolorized heatmap
jet_heatmap = keras.utils.array_to_img(jet_heatmap)
jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))
jet_heatmap = keras.utils.img_to_array(jet_heatmap)

# Superimpose the heatmap and the original image, with the heatmap at 40% opacity
superimposed_img = jet_heatmap * 0.4 + img
superimposed_img = keras.utils.array_to_img(superimposed_img)

# Save the superimposed image
save_path = "elephant_cam.jpg"
superimposed_img.save(save_path)

```

![heatmap superimposed on original picture](elephant_cam.jpg "Figure 9.20 African elephant class activation heatmap over the test picture")

<!-- #region colab_type="text" -->
## Summary
<!-- #endregion -->
