---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.0
  kernelspec:
    display_name: TF2.6Py3.7
    language: python
    name: tf2.6py3.7
---

<!-- #region colab_type="text" -->
This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.

**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**

This notebook was generated for TensorFlow 2.6.
<!-- #endregion -->

```{python}
# Get current python environment - depends on selected kernel
import os
import sys
print("os.getcwd(): " + os.getcwd())
print("sys.prefix: ", sys.prefix)
print("sys.exec_prefix: ", sys.exec_prefix)
print("sys.executable: ", sys.executable)
print("os.path.basename(sys.exec_prefix)", os.path.basename(sys.exec_prefix))

import tensorflow as tf
print(f"TensorFlow Version: {tf.__version__}")

if tf.__version__ >= "2.":
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    tf.config.list_physical_devices('GPU')
    import tensorflow.keras
    print(f"Keras Version: {tensorflow.keras.__version__}")
    gpu = len(tf.config.list_physical_devices('GPU'))>0
    print("GPU is", "available" if gpu else "NOT AVAILABLE")
    if gpu:
        print(tf.config.list_physical_devices('GPU'))
        
```

```{python}
# Get installed package versions in virtual environment associated to Jupyter kernel
import pkg_resources

pkgs = dict([(d.__dict__.get("_key"), d.__dict__.get("_version")) for d in pkg_resources.working_set])

for k in sorted(pkgs.keys()):
    print(k, pkgs.get(k))
```

```{python}
import subprocess
import sys
import pkg_resources


# Calling [sys.executable, '-m', 'pip', 'install', name] 
# rather than ['pip', 'install', name]
# is making sure to get the "right" pip (i.e. you install in current virtual environment)
def install(package):
    # Get installed package versions in virtual environment associated to Jupyter kernel
    pkgs = dict([(d.__dict__.get("_key"), d.__dict__.get("_version")) for d in pkg_resources.working_set])
    if package not in pkgs.keys():
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    else:
        print(f"Package {package} version {pkgs.get(package)} is already installed")
    
install("ipython-autotime")
# %load_ext autotime
```

<!-- #region colab_type="text" -->
## Interpreting what convnets learn
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Visualizing intermediate activations
<!-- #endregion -->

```{python colab_type="code"}
# You can use this to load the file "convnet_from_scratch_with_augmentation.keras"
# you obtained in the last chapter.
# from google.colab import files
# files.upload()
```

```{python colab_type="code"}
from tensorflow import keras
from GAmodel import GAModel

#model = keras.models.load_model("convnet_from_scratch_with_augmentation.keras")
model = keras.models.load_model("convnet_from_scratch_with_augmentation.tf")
```

```{python colab_type="code"}
model.summary()
```

<!-- #region colab_type="text" -->
**Preprocessing a single image**
<!-- #endregion -->

```{python colab_type="code"}
from tensorflow import keras
import numpy as np

img_path = keras.utils.get_file(
    fname="cat.jpg",
    origin="https://img-datasets.s3.amazonaws.com/cat.jpg")

def get_img_array(img_path, target_size):
    img = keras.utils.load_img(
        img_path, target_size=target_size)
    array = keras.utils.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    return array

img_tensor = get_img_array(img_path, target_size=(180, 180))
```

<!-- #region colab_type="text" -->
**Displaying the test picture**
<!-- #endregion -->

```{python colab_type="code"}
import matplotlib.pyplot as plt
plt.axis("off")
plt.imshow(img_tensor[0].astype("uint8"))
plt.show()
```

<!-- #region colab_type="text" -->
**Instantiating a model that returns layer activations**
<!-- #endregion -->

```{python colab_type="code"}
from tensorflow.keras import layers

layer_outputs = []
layer_names = []
for layer in model.layers:
    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):
        layer_outputs.append(layer.output)
        layer_names.append(layer.name)
activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)
```

<!-- #region colab_type="text" -->
**Using the model to compute layer activations**
<!-- #endregion -->

```{python colab_type="code"}
activations = activation_model.predict(img_tensor)
```

```{python colab_type="code"}
first_layer_activation = activations[0]
print(first_layer_activation.shape)
```

<!-- #region colab_type="text" -->
**Visualizing the fifth channel**
<!-- #endregion -->

```{python colab_type="code"}
import matplotlib.pyplot as plt
plt.matshow(first_layer_activation[0, :, :, 5], cmap="viridis")
```

<!-- #region colab_type="text" -->
**Visualizing every channel in every intermediate activation**
<!-- #endregion -->

```{python colab_type="code"}
images_per_row = 16
for layer_name, layer_activation in zip(layer_names, activations):
    n_features = layer_activation.shape[-1]
    size = layer_activation.shape[1]
    n_cols = n_features // images_per_row
    display_grid = np.zeros(((size + 1) * n_cols - 1,
                             images_per_row * (size + 1) - 1))
    for col in range(n_cols):
        for row in range(images_per_row):
            channel_index = col * images_per_row + row
            channel_image = layer_activation[0, :, :, channel_index].copy()
            if channel_image.sum() != 0:
                channel_image -= channel_image.mean()
                channel_image /= channel_image.std()
                channel_image *= 64
                channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype("uint8")
            display_grid[
                col * (size + 1): (col + 1) * size + col,
                row * (size + 1) : (row + 1) * size + row] = channel_image
    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.axis("off")
    plt.imshow(display_grid, aspect="auto", cmap="viridis")
```

<!-- #region colab_type="text" -->
### Visualizing convnet filters
<!-- #endregion -->

<!-- #region colab_type="text" -->
**Instantiating the Xception convolutional base**
<!-- #endregion -->

```{python colab_type="code"}
model = keras.applications.xception.Xception(
    weights="imagenet",
    include_top=False)
```

<!-- #region colab_type="text" -->
**Printing the names of all convolutional layers in Xception**
<!-- #endregion -->

```{python colab_type="code"}
for layer in model.layers:
    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):
        print(layer.name)
```

<!-- #region colab_type="text" -->
**Creating a feature extractor model**
<!-- #endregion -->

```{python colab_type="code"}
layer_name = "block3_sepconv1"
layer = model.get_layer(name=layer_name)
feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)
```

<!-- #region colab_type="text" -->
**Using the feature extractor**
<!-- #endregion -->

```{python colab_type="code"}
activation = feature_extractor(
    keras.applications.xception.preprocess_input(img_tensor)
)
```

```{python colab_type="code"}
import tensorflow as tf

def compute_loss(image, filter_index):
    activation = feature_extractor(image)
    filter_activation = activation[:, 2:-2, 2:-2, filter_index]
    return tf.reduce_mean(filter_activation)
```

<!-- #region colab_type="text" -->
**Loss maximization via stochastic gradient ascent**
<!-- #endregion -->

```{python colab_type="code"}
@tf.function
def gradient_ascent_step(image, filter_index, learning_rate):
    with tf.GradientTape() as tape:
        tape.watch(image)
        loss = compute_loss(image, filter_index)
    grads = tape.gradient(loss, image)
    grads = tf.math.l2_normalize(grads)
    image += learning_rate * grads
    return image
```

<!-- #region colab_type="text" -->
**Function to generate filter visualizations**
<!-- #endregion -->

```{python colab_type="code"}
img_width = 200
img_height = 200

def generate_filter_pattern(filter_index):
    iterations = 30
    learning_rate = 10.
    image = tf.random.uniform(
        minval=0.4,
        maxval=0.6,
        shape=(1, img_width, img_height, 3))
    for i in range(iterations):
        image = gradient_ascent_step(image, filter_index, learning_rate)
    return image[0].numpy()
```

<!-- #region colab_type="text" -->
**Utility function to convert a tensor into a valid image**
<!-- #endregion -->

```{python colab_type="code"}
def deprocess_image(image):
    image -= image.mean()
    image /= image.std()
    image *= 64
    image += 128
    image = np.clip(image, 0, 255).astype("uint8")
    image = image[25:-25, 25:-25, :]
    return image
```

```{python colab_type="code"}
plt.axis("off")
plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))
```

<!-- #region colab_type="text" -->
**Generating a grid of all filter response patterns in a layer**
<!-- #endregion -->

```{python colab_type="code"}
all_images = []
for filter_index in range(64):
    print(f"Processing filter {filter_index}")
    image = deprocess_image(
        generate_filter_pattern(filter_index)
    )
    all_images.append(image)

margin = 5
n = 8
cropped_width = img_width - 25 * 2
cropped_height = img_height - 25 * 2
width = n * cropped_width + (n - 1) * margin
height = n * cropped_height + (n - 1) * margin
stitched_filters = np.zeros((width, height, 3))

for i in range(n):
    for j in range(n):
        image = all_images[i * n + j]
        stitched_filters[
            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,
            (cropped_height + margin) * j : (cropped_height + margin) * j
            + cropped_height,
            :,
        ] = image

keras.utils.save_img(
    f"filters_for_layer_{layer_name}.png", stitched_filters)
print(f"filters_for_layer_{layer_name}.png")
```

![filters for layer predictions](filters_for_layer_predictions.png "filters_for_layer_predictions.png")

<!-- #region colab_type="text" -->
### Visualizing heatmaps of class activation
<!-- #endregion -->

<!-- #region colab_type="text" -->
**Loading the Xception network with pretrained weights**
<!-- #endregion -->

```{python colab_type="code"}
model = keras.applications.xception.Xception(weights="imagenet")
```

<!-- #region colab_type="text" -->
**Preprocessing an input image for Xception**
<!-- #endregion -->

```{python colab_type="code"}
img_path = keras.utils.get_file(
    fname="elephant.jpg",
    origin="https://img-datasets.s3.amazonaws.com/elephant.jpg")

print(img_path)

def get_img_array(img_path, target_size):
    img = keras.utils.load_img(img_path, target_size=target_size)
    array = keras.utils.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    array = keras.applications.xception.preprocess_input(array)
    return array

img_array = get_img_array(img_path, target_size=(299, 299))
```

![original picture](elephant.jpg "elephant.jpg")

```{python colab_type="code"}
preds = model.predict(img_array)
print(keras.applications.xception.decode_predictions(preds, top=3)[0])
```

```{python colab_type="code"}
np.argmax(preds[0])
```

<!-- #region colab_type="text" -->
**Setting up a model that returns the last convolutional output**
<!-- #endregion -->

```{python colab_type="code"}
last_conv_layer_name = "block14_sepconv2_act"
classifier_layer_names = [
    "avg_pool",
    "predictions",
]
last_conv_layer = model.get_layer(last_conv_layer_name)
last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)
```

<!-- #region colab_type="text" -->
**Reapplying the classifier on top of the last convolutional output**
<!-- #endregion -->

```{python colab_type="code"}
classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])
x = classifier_input
for layer_name in classifier_layer_names:
    x = model.get_layer(layer_name)(x)
classifier_model = keras.Model(classifier_input, x)
```

<!-- #region colab_type="text" -->
**Retrieving the gradients of the top predicted class**
<!-- #endregion -->

```{python colab_type="code"}
import tensorflow as tf

with tf.GradientTape() as tape:
    last_conv_layer_output = last_conv_layer_model(img_array)
    tape.watch(last_conv_layer_output)
    preds = classifier_model(last_conv_layer_output)
    top_pred_index = tf.argmax(preds[0])
    top_class_channel = preds[:, top_pred_index]

grads = tape.gradient(top_class_channel, last_conv_layer_output)
```

<!-- #region colab_type="text" -->
**Gradient pooling and channel-importance weighting**
<!-- #endregion -->

```{python colab_type="code"}
pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()
last_conv_layer_output = last_conv_layer_output.numpy()[0]
for i in range(pooled_grads.shape[-1]):
    last_conv_layer_output[:, :, i] *= pooled_grads[i]
heatmap = np.mean(last_conv_layer_output, axis=-1)
```

<!-- #region colab_type="text" -->
**Heatmap post-processing**
<!-- #endregion -->

```{python colab_type="code"}
heatmap = np.maximum(heatmap, 0)
heatmap /= np.max(heatmap)
plt.matshow(heatmap)
```

<!-- #region colab_type="text" -->
**Superimposing the heatmap on the original picture**
<!-- #endregion -->

```{python colab_type="code"}
import matplotlib.cm as cm

img = keras.utils.load_img(img_path)
img = keras.utils.img_to_array(img)

heatmap = np.uint8(255 * heatmap)

jet = cm.get_cmap("jet")
jet_colors = jet(np.arange(256))[:, :3]
jet_heatmap = jet_colors[heatmap]

jet_heatmap = keras.utils.array_to_img(jet_heatmap)
jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))
jet_heatmap = keras.utils.img_to_array(jet_heatmap)

superimposed_img = jet_heatmap * 0.4 + img
superimposed_img = keras.utils.array_to_img(superimposed_img)

save_path = "elephant_cam.jpg"
superimposed_img.save(save_path)
```

![heatmap superimposed on original picture](elephant_cam.jpg "elephant_cam.jpg")

<!-- #region colab_type="text" -->
## Summary
<!-- #endregion -->
