---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region colab_type="text" -->
This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.

**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**

This notebook was generated for TensorFlow 2.6.
<!-- #endregion -->

<!-- #region colab_type="text" -->
# Introduction to Keras and TensorFlow
<!-- #endregion -->

<!-- #region colab_type="text" -->
## What's TensorFlow?
<!-- #endregion -->

<!-- #region colab_type="text" -->
## What's Keras?
<!-- #endregion -->

<!-- #region colab_type="text" -->
## Keras and TensorFlow: A brief history
<!-- #endregion -->

<!-- #region colab_type="text" -->
## Setting up a deep-learning workspace
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Jupyter notebooks: The preferred way to run deep-learning experiments
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Using Colaboratory
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### First steps with Colaboratory
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Installing packages with pip
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Using the GPU runtime
<!-- #endregion -->

<!-- #region colab_type="text" -->
## First steps with TensorFlow
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Constant tensors and variables
<!-- #endregion -->

<!-- #region colab_type="text" -->
**All-ones or all-zeros tensors**
<!-- #endregion -->

```{python colab_type="code"}
import tensorflow as tf
x = tf.ones(shape=(2, 1))
print(x)
```

```{python colab_type="code"}
x = tf.zeros(shape=(2, 1))
print(x)
```

<!-- #region colab_type="text" -->
**Random tensors**
<!-- #endregion -->

```{python colab_type="code"}
x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)
print(x)
```

```{python colab_type="code"}
x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.)
print(x)
```

<!-- #region colab_type="text" -->
**NumPy arrays are assignable**
<!-- #endregion -->

```{python colab_type="code"}
import numpy as np
x = np.ones(shape=(2, 2))
x[0, 0] = 0.
```

<!-- #region colab_type="text" -->
**Creating a TensorFlow variable**
<!-- #endregion -->

```{python colab_type="code"}
v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
print(v)
```

<!-- #region colab_type="text" -->
**Assigning a value to a TensorFlow variable**
<!-- #endregion -->

```{python colab_type="code"}
v.assign(tf.ones((3, 1)))
```

<!-- #region colab_type="text" -->
**Assigning a value to a subset of a TensorFlow variable**
<!-- #endregion -->

```{python colab_type="code"}
v[0, 0].assign(3.)
```

<!-- #region colab_type="text" -->
**Using `assign_add`**
<!-- #endregion -->

```{python colab_type="code"}
v.assign_add(tf.ones((3, 1)))
```

<!-- #region colab_type="text" -->
#### Tensor operations: Doing math in TensorFlow
<!-- #endregion -->

<!-- #region colab_type="text" -->
**A few basic math operations**
<!-- #endregion -->

```{python colab_type="code"}
a = tf.ones((2, 2))
b = tf.square(a)
c = tf.sqrt(a)
d = b + c
e = tf.matmul(a, b)
e *= d
```

<!-- #region colab_type="text" -->
#### A second look at the GradientTape API
<!-- #endregion -->

<!-- #region colab_type="text" -->
**Using the `GradientTape`**
<!-- #endregion -->

```{python colab_type="code"}
input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
   result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
```

<!-- #region colab_type="text" -->
**Using `GradientTape` with constant tensor inputs**
<!-- #endregion -->

```{python colab_type="code"}
input_const = tf.constant(3.)
with tf.GradientTape() as tape:
   tape.watch(input_const)
   result = tf.square(input_const)
gradient = tape.gradient(result, input_const)
```

<!-- #region colab_type="text" -->
**Using nested gradient tapes to compute second-order gradients**
<!-- #endregion -->

```{python colab_type="code"}
time = tf.Variable(0.)
with tf.GradientTape() as outer_tape:
    with tf.GradientTape() as inner_tape:
        position =  4.9 * time ** 2
    speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
```

<!-- #region colab_type="text" -->
#### An end-to-end example: A linear classifier in pure TensorFlow
<!-- #endregion -->

<!-- #region colab_type="text" -->
**Generating two classes of random points in a 2D plane**
<!-- #endregion -->

```{python colab_type="code"}
num_samples_per_class = 1000
negative_samples = np.random.multivariate_normal(
    mean=[0, 3],
    cov=[[1, 0.5],[0.5, 1]],
    size=num_samples_per_class)
positive_samples = np.random.multivariate_normal(
    mean=[3, 0],
    cov=[[1, 0.5],[0.5, 1]],
    size=num_samples_per_class)
```

<!-- #region colab_type="text" -->
**Stacking the two classes into an array with shape (2000, 2)**
<!-- #endregion -->

```{python colab_type="code"}
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)
```

<!-- #region colab_type="text" -->
**Generating the corresponding targets (0 and 1)**
<!-- #endregion -->

```{python colab_type="code"}
targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype="float32"),
                     np.ones((num_samples_per_class, 1), dtype="float32")))
```

<!-- #region colab_type="text" -->
**Plotting the two point classes**
<!-- #endregion -->

```{python colab_type="code"}
import matplotlib.pyplot as plt
plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])
plt.show()
```

<!-- #region colab_type="text" -->
**Creating the linear classifier variables**
<!-- #endregion -->

```{python colab_type="code"}
input_dim = 2
output_dim = 1
W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))
b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))
```

<!-- #region colab_type="text" -->
**The forward pass function**
<!-- #endregion -->

```{python colab_type="code"}
def model(inputs):
    return tf.matmul(inputs, W) + b
```

<!-- #region colab_type="text" -->
**The mean squared error loss function**
<!-- #endregion -->

```{python colab_type="code"}
def square_loss(targets, predictions):
    per_sample_losses = tf.square(targets - predictions)
    return tf.reduce_mean(per_sample_losses)
```

<!-- #region colab_type="text" -->
**The training step function**
<!-- #endregion -->

```{python colab_type="code"}
learning_rate = 0.1

def training_step(inputs, targets):
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = square_loss(predictions, targets)
    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
    W.assign_sub(grad_loss_wrt_W * learning_rate)
    b.assign_sub(grad_loss_wrt_b * learning_rate)
    return loss
```

<!-- #region colab_type="text" -->
**The batch training loop**
<!-- #endregion -->

```{python colab_type="code"}
for step in range(40):
    loss = training_step(inputs, targets)
    print(f"Loss at step {step}: {loss:.4f}")
```

```{python colab_type="code"}
predictions = model(inputs)
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)
plt.show()
```

```{python colab_type="code"}
x = np.linspace(-1, 4, 100)
y = - W[0] /  W[1] * x + (0.5 - b) / W[1]
plt.plot(x, y, "-r")
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)
```

<!-- #region colab_type="text" -->
## Anatomy of a neural network: Understanding core Keras APIs
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Layers: The building blocks of deep learning
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### The base Layer class in Keras
<!-- #endregion -->

<!-- #region colab_type="text" -->
**A `Dense` layer implemented as a `Layer` subclass**
<!-- #endregion -->

```{python colab_type="code"}
from tensorflow import keras

class SimpleDense(keras.layers.Layer):

    def __init__(self, units, activation=None):
        super().__init__()
        self.units = units
        self.activation = activation

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.W = self.add_weight(shape=(input_dim, self.units),
                                 initializer="random_normal")
        self.b = self.add_weight(shape=(self.units,),
                                 initializer="zeros")

    def call(self, inputs):
        y = tf.matmul(inputs, self.W) + self.b
        if self.activation is not None:
            y = self.activation(y)
        return y
```

```{python colab_type="code"}
my_dense = SimpleDense(units=32, activation=tf.nn.relu)
input_tensor = tf.ones(shape=(2, 784))
output_tensor = my_dense(input_tensor)
print(output_tensor.shape)
```

<!-- #region colab_type="text" -->
#### Automatic shape inference: Building layers on the fly
<!-- #endregion -->

```{python colab_type="code"}
from tensorflow.keras import layers
layer = layers.Dense(32, activation="relu")
```

```{python colab_type="code"}
from tensorflow.keras import models
from tensorflow.keras import layers
model = models.Sequential([
    layers.Dense(32, activation="relu"),
    layers.Dense(32)
])
```

```{python colab_type="code"}
model = keras.Sequential([
    SimpleDense(32, activation="relu"),
    SimpleDense(64, activation="relu"),
    SimpleDense(32, activation="relu"),
    SimpleDense(10, activation="softmax")
])
```

<!-- #region colab_type="text" -->
### From layers to models
<!-- #endregion -->

<!-- #region colab_type="text" -->
### The "compile" step: Configuring the learning process
<!-- #endregion -->

```{python colab_type="code"}
model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer="rmsprop",
              loss="mean_squared_error",
              metrics=["accuracy"])
```

```{python colab_type="code"}
model.compile(optimizer=keras.optimizers.RMSprop(),
              loss=keras.losses.MeanSquaredError(),
              metrics=[keras.metrics.BinaryAccuracy()])
```

<!-- #region colab_type="text" -->
### Picking a loss function
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Understanding the fit() method
<!-- #endregion -->

<!-- #region colab_type="text" -->
**Calling `fit()` with NumPy data**
<!-- #endregion -->

```{python colab_type="code"}
history = model.fit(
    inputs,
    targets,
    epochs=5,
    batch_size=128
)
```

```{python colab_type="code"}
history.history
```

<!-- #region colab_type="text" -->
### Monitoring loss and metrics on validation data
<!-- #endregion -->

<!-- #region colab_type="text" -->
**Using the `validation_data` argument**
<!-- #endregion -->

```{python colab_type="code"}
model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),
              loss=keras.losses.MeanSquaredError(),
              metrics=[keras.metrics.BinaryAccuracy()])

indices_permutation = np.random.permutation(len(inputs))
shuffled_inputs = inputs[indices_permutation]
shuffled_targets = targets[indices_permutation]

num_validation_samples = int(0.3 * len(inputs))
val_inputs = shuffled_inputs[:num_validation_samples]
val_targets = shuffled_targets[:num_validation_samples]
training_inputs = shuffled_inputs[num_validation_samples:]
training_targets = shuffled_targets[num_validation_samples:]
model.fit(
    training_inputs,
    training_targets,
    epochs=5,
    batch_size=16,
    validation_data=(val_inputs, val_targets)
)
```

<!-- #region colab_type="text" -->
### Inference: Using a model after training
<!-- #endregion -->

```{python colab_type="code"}
predictions = model.predict(val_inputs, batch_size=128)
print(predictions[:10])
```

<!-- #region colab_type="text" -->
## Summary
<!-- #endregion -->
