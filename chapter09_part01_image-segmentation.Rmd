---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.0
  kernelspec:
    display_name: TF2.6Py3.7
    language: python
    name: tf2.6py3.7
---

<!-- #region colab_type="text" -->
This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.

**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**

This notebook was generated for TensorFlow 2.6.
<!-- #endregion -->

```{python}
import os
import sys
print("os.getcwd(): " + os.getcwd())
print("sys.prefix: ", sys.prefix)
print("sys.exec_prefix: ", sys.exec_prefix)
print("sys.executable: ", sys.executable)
print("os.path.basename(sys.exec_prefix)", os.path.basename(sys.exec_prefix))

import tensorflow as tf
print(f"TensorFlow Version: {tf.__version__}")

if tf.__version__ >= "2.":
    gpus = tf.config.experimental.list_physical_devices('GPU')
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
    tf.config.list_physical_devices('GPU')
    import tensorflow.keras
    print(f"Keras Version: {tensorflow.keras.__version__}")
    gpu = len(tf.config.list_physical_devices('GPU'))>0
    print("GPU is", "available" if gpu else "NOT AVAILABLE")
    if gpu:
        print(tf.config.list_physical_devices('GPU'))
        

```

<!-- #region colab_type="text" -->
# 9 Advanced deep learning for computer vision
<!-- #endregion -->

<!-- #region colab_type="text" -->
## 9.1 Three essential computer vision tasks
<!-- #endregion -->

<!-- #region colab_type="text" -->
## 9.2 An image segmentation example

Download and uncompress our dataset:
<!-- #endregion -->

```{python}
import subprocess
import sys

# Calling [sys.executable, '-m', 'pip', 'install', name] 
# rather than ['pip', 'install', name]
# is making sure to get the "right" pip (i.e. you install in current virtual environment)
def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    
install("ipython-autotime")
# %load_ext autotime
```

```{python colab_type="code"}
! [ ! -e images.tar.gz ]  && wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
! [ ! -e annotations.tar.gz ] && wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
# !tar -xf images.tar.gz
# !tar -xf annotations.tar.gz
```

```{python colab_type="code"}
import os

input_dir = "images/"
target_dir = "annotations/trimaps/"

input_img_paths = sorted(
    [os.path.join(input_dir, fname)
     for fname in os.listdir(input_dir)
     if fname.endswith(".jpg")])
print(type(input_img_paths))
print(len(input_img_paths))
print(type(input_img_paths[0]))
```

```{python colab_type="code"}
target_paths = sorted(
    [os.path.join(target_dir, fname)
     for fname in os.listdir(target_dir)
     if fname.endswith(".png") and not fname.startswith(".")])

print(type(target_paths))
print(len(target_paths))
print(type(target_paths[0]))
```

```{python colab_type="code"}
import matplotlib.pyplot as plt
from tensorflow.keras.utils import load_img, img_to_array

rawimg = load_img(input_img_paths[9])  # Display input image number 9.
print(type(rawimg))

#plt.axis("off")
plt.imshow(rawimg)
```

```{python colab_type="code"}
def display_target(target_array):
    normalized_array = (target_array.astype("uint8") - 1) * 127 # The original labels are 1, 2, and 3. We subtract 1 so that the
                                                                # labels range from 0 to 2, and then we multiply by 127 so that
                                                                # the labels become 0 (black), 127 (gray), 254 (near-white).
    print(normalized_array.min(), normalized_array.max())
    #plt.axis("off")
    plt.imshow(normalized_array[:, :, 0])
    plt.show()
    plt.imshow(normalized_array[:, :, 0], cmap='gray', vmin=0, vmax=255)

rawtarget = load_img(target_paths[9], color_mode="grayscale")   # Display target (mask) number 9.
                                                                # The image is treated as having a single color channel.
print(type(rawtarget))
targimg = img_to_array(rawtarget)
print(type(targimg))
print(targimg.shape)
print(targimg.min(), targimg.max())
display_target(targimg)
```

```{python colab_type="code"}
import numpy as np
import random

img_size = (200, 200)              # we will resize the images to these dimensions
num_imgs = len(input_img_paths)

# Shuffle the file paths (originally sorted by breed) using same seed (1337) for both images and targets to keep them aligned
random.Random(1337).shuffle(input_img_paths)
random.Random(1337).shuffle(target_paths)

def path_to_input_image(path):
    return img_to_array(load_img(path, target_size=img_size))
print("returned image shape:", path_to_input_image(input_img_paths[0]).shape)

def path_to_target(path):
    img = img_to_array(load_img(path, target_size=img_size, color_mode="grayscale"))
    img = img.astype("uint8") - 1   # Subtract 1 so that our labels become 0, 1, and 2.
    return img
print("returned target shape:", path_to_target(target_paths[0]).shape)

input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype="float32")
print("input_imgs.shape:", input_imgs.shape)
targets = np.zeros((num_imgs,) + img_size + (1,), dtype="uint8")
print("targets.shape:", targets.shape)

for i in range(num_imgs):
    input_imgs[i] = path_to_input_image(input_img_paths[i])
    targets[i] = path_to_target(target_paths[i])

# Split into training and validation sets
num_val_samples = 1000
train_input_imgs = input_imgs[:-num_val_samples]
train_targets = targets[:-num_val_samples]
val_input_imgs = input_imgs[-num_val_samples:]
val_targets = targets[-num_val_samples:]
```

```{python colab_type="code"}
from tensorflow import keras
from tensorflow.keras import layers
from GAmodel import GAModel

def get_model(img_size, num_classes, n_gradients = 1):
    inputs = keras.Input(shape=img_size + (3,))
    x = layers.Rescaling(1./255)(inputs)   # rescale input images to the [0-1] range.

    # First half of the model:
    # Use padding="same" everywhere to avoid the influence of border padding on feature map size
    # Stack Conv2D layers, with gradually increasing filter sizes, down-sampling our images three times 
    # by a factor of two each, ending up with activations of size (25, 25, 256)
    # Purpose: encode the images into smaller feature maps, where each spatial location (or pixel) 
    # contains information about a large spatial chunk of the original image (kind of compression).
    # We downsample by adding strides rather than max pooling, because we care a lot about the spatial 
    # location of information in the image.
    # We tend to use strides instead of max pooling in any model that cares about feature location.
    # The output of this first half of the model is a feature map of shape (25, 25, 256).
    x = layers.Conv2D(64, 3, strides=2, activation="relu", padding="same")(x)
    x = layers.Conv2D(64, 3, activation="relu", padding="same")(x)
    x = layers.Conv2D(128, 3, strides=2, activation="relu", padding="same")(x)
    x = layers.Conv2D(128, 3, activation="relu", padding="same")(x)
    x = layers.Conv2D(256, 3, strides=2, padding="same", activation="relu")(x)
    x = layers.Conv2D(256, 3, activation="relu", padding="same")(x)

    # Second half of the model:
    # Stack Conv2DTranspose layers: this will up-sample the feature maps
    # until we get back to the original image size
    x = layers.Conv2DTranspose(256, 3, activation="relu", padding="same")(x)
    x = layers.Conv2DTranspose(256, 3, activation="relu", padding="same", strides=2)(x)
    x = layers.Conv2DTranspose(128, 3, activation="relu", padding="same")(x)
    x = layers.Conv2DTranspose(128, 3, activation="relu", padding="same", strides=2)(x)
    x = layers.Conv2DTranspose(64, 3, activation="relu", padding="same")(x)
    x = layers.Conv2DTranspose(64, 3, activation="relu", padding="same", strides=2)(x)

    # We end the model with a per-pixel three-way softmax to classify each output pixel 
    # into one of our three categories
    outputs = layers.Conv2D(num_classes, 3, activation="softmax", padding="same")(x)

    if n_gradients == 1:
        model = keras.Model(inputs, outputs)
    else:
        model = GAModel(inputs, outputs, n_gradients = n_gradients)
    return model
```

```{python colab_type="code"}
# # %load_ext autotime
# model = get_model(img_size=img_size, num_classes=3, n_gradients = 1)
# model.summary()
```

```{python colab_type="code"}
# model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy")

# callbacks = [
#     keras.callbacks.ModelCheckpoint("oxford_segmentation.keras", save_best_only=True)
# ]

# history = model.fit(train_input_imgs, train_targets,
#                     epochs=50,
#                     callbacks=callbacks,
#                     batch_size=8, # we run out of memory with batch size 64, 32 or 16
#                     validation_data=(val_input_imgs, val_targets))
```

```{python colab_type="code"}
# epochs = range(1, len(history.history["loss"]) + 1)
# loss = history.history["loss"]
# val_loss = history.history["val_loss"]
# plt.figure()
# plt.plot(epochs, loss, "bo", label="Training loss")
# plt.plot(epochs, val_loss, "b", label="Validation loss")
# plt.ylim(0,1)
# plt.title("Training and validation loss")
# plt.legend()
```

#### -- Using Accumulated Gradients to mimick larger batch size with limited GPU memory --

```{python}
# Accumulate Gradients before updating weights - effectively mimicking larger batch size with limited GPU memory
# %load_ext autotime
model = get_model(img_size=img_size, num_classes=3, n_gradients = 8)
model.summary()
```

```{python}
model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy")

callbacks = [
    keras.callbacks.ModelCheckpoint("oxford_segmentation.tf", save_best_only=True)
]

history = model.fit(train_input_imgs, train_targets,
                    epochs=50,
                    callbacks=callbacks,
                    batch_size=8, # we run out of memory with batch size 64, 32 or 16
                    validation_data=(val_input_imgs, val_targets))
```

```{python}
# %load_ext autotime
epochs = range(1, len(history.history["loss"]) + 1)
loss = history.history["loss"]
val_loss = history.history["val_loss"]
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.ylim(0.15,1.15)
plt.title("Training and validation loss")
plt.legend()
```

*we start overfitting midway, around epoch 25.*

**-- Predict a segmentation mask --**

```{python colab_type="code"}
from tensorflow.keras.utils import array_to_img

model = keras.models.load_model("oxford_segmentation.tf")

i = 4
test_image = val_input_imgs[i].copy()
plt.axis("off")
plt.imshow(array_to_img(test_image))
plt.show()

mask = model.predict(np.expand_dims(test_image, 0))[0]

def display_mask(pred):
    mask = np.argmax(pred, axis=-1)
    mask *= 127
    plt.axis("off")
    plt.imshow(mask)

display_mask(mask)
```

```{python}
print(test_image.shape)
print(mask.shape)
print(mask.min(), mask.max())
```

```{python}
maskb = np.argmax(mask, axis=-1)
print(maskb.shape, maskb.min(), maskb.max())
plt.imshow(maskb != 0)
plt.show()
plt.imshow(maskb != 1)
plt.show()
plt.imshow(maskb != 2)
plt.show()

```

```{python}
for c in range(3):
    test_image[:,:,c] = test_image[:,:,c] * (maskb != 1)

plt.axis("off")
plt.imshow(array_to_img(test_image))
plt.show()
```

```{python}

```

```{python}

```
