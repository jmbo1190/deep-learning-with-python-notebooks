---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.8.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region colab_type="text" -->
This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.

**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**

This notebook was generated for TensorFlow 2.6.
<!-- #endregion -->

<!-- #region colab_type="text" -->
# Conclusions
<!-- #endregion -->

<!-- #region colab_type="text" -->
## Key concepts in review
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Various approaches to AI
<!-- #endregion -->

<!-- #region colab_type="text" -->
### What makes deep learning special within the field of machine learning
<!-- #endregion -->

<!-- #region colab_type="text" -->
### How to think about deep learning
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Key enabling technologies
<!-- #endregion -->

<!-- #region colab_type="text" -->
### The universal machine-learning workflow
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Key network architectures
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Densely connected networks
<!-- #endregion -->

```{python colab_type="code"}
from tensorflow import keras
from tensorflow.keras import layers
inputs = keras.Input(shape=(num_input_features,))
x = layers.Dense(32, activation="relu")(inputs)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
```

```{python colab_type="code"}
inputs = keras.Input(shape=(num_input_features,))
x = layers.Dense(32, activation="relu")(inputs)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="categorical_crossentropy")
```

```{python colab_type="code"}
inputs = keras.Input(shape=(num_input_features,))
x = layers.Dense(32, activation="relu")(inputs)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(num_classes, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
```

```{python colab_type="code"}
inputs = keras.Input(shape=(num_input_features,))
x = layers.Dense(32, activation="relu")(inputs)
x = layers.Dense(32, activation="relu")(x)
outputs layers.Dense(num_values)(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="mse")
```

<!-- #region colab_type="text" -->
#### Convnets
<!-- #endregion -->

```{python colab_type="code"}
inputs = keras.Input(shape=(height, width, channels))
x = layers.SeparableConv2D(32, 3, activation="relu")(inputs)
x = layers.SeparableConv2D(64, 3, activation="relu")(x)
x = layers.MaxPooling2D(2)(x)
x = layers.SeparableConv2D(64, 3, activation="relu")(x)
x = layers.SeparableConv2D(128, 3, activation="relu")(x)
x = layers.MaxPooling2D(2)(x)
x = layers.SeparableConv2D(64, 3, activation="relu")(x)
x = layers.SeparableConv2D(128, 3, activation="relu")(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(32, activation="relu")(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="categorical_crossentropy")
```

<!-- #region colab_type="text" -->
#### RNNs
<!-- #endregion -->

```{python colab_type="code"}
inputs = keras.Input(shape=(num_timesteps, num_features))
x = layers.LSTM(32)(inputs)
outputs = layers.Dense(num_classes, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
```

```{python colab_type="code"}
inputs = keras.Input(shape=(num_timesteps, num_features))
x = layers.LSTM(32, return_sequences=True)(inputs)
x = layers.LSTM(32, return_sequences=True)(x)
x = layers.LSTM(32)(x)
outputs = layers.Dense(num_classes, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
```

<!-- #region colab_type="text" -->
#### Transformers
<!-- #endregion -->

```{python colab_type="code"}
encoder_inputs = keras.Input(shape=(sequence_length,), dtype="int64")
x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)
encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
decoder_inputs = keras.Input(shape=(None,), dtype="int64")
x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)
x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)
decoder_outputs = layers.Dense(vocab_size, activation="softmax")(x)
transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
transformer.compile(optimizer="rmsprop", loss="categorical_crossentropy")
```

```{python colab_type="code"}
inputs = keras.Input(shape=(sequence_length,), dtype="int64")
x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)
x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
x = layers.GlobalMaxPooling1D()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop", loss="binary_crossentropy")
```

<!-- #region colab_type="text" -->
### The space of possibilities
<!-- #endregion -->

<!-- #region colab_type="text" -->
## The limitations of deep learning
<!-- #endregion -->

<!-- #region colab_type="text" -->
### The risk of anthropomorphizing machine-learning models
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Automatons vs. intelligent agents
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Local generalization vs. extreme generalization
<!-- #endregion -->

<!-- #region colab_type="text" -->
### The purpose of intelligence
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Climbing the spectrum of generalization
<!-- #endregion -->

<!-- #region colab_type="text" -->
## Setting the course toward greater generality in AI
<!-- #endregion -->

<!-- #region colab_type="text" -->
### On the importance of setting the right objective: The shortcut rule
<!-- #endregion -->

<!-- #region colab_type="text" -->
### A new target
<!-- #endregion -->

<!-- #region colab_type="text" -->
## Implementing intelligence: The missing ingredients
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Intelligence as sensitivity to abstract analogies
<!-- #endregion -->

<!-- #region colab_type="text" -->
### The two poles of abstraction
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Value-centric analogy
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Program-centric analogy
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Cognition as a combination of both kinds of abstraction
<!-- #endregion -->

<!-- #region colab_type="text" -->
### The missing half of the picture
<!-- #endregion -->

<!-- #region colab_type="text" -->
## The future of deep learning
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Models as programs
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Blending together deep learning and program synthesis
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Integrating deep-learning modules and algorithmic modules into hybrid systems
<!-- #endregion -->

<!-- #region colab_type="text" -->
#### Using deep learning to guide program search
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Lifelong learning and modular subroutine reuse
<!-- #endregion -->

<!-- #region colab_type="text" -->
### The long-term vision
<!-- #endregion -->

<!-- #region colab_type="text" -->
## Staying up to date in a fast-moving field
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Practice on real-world problems using Kaggle
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Read about the latest developments on arXiv
<!-- #endregion -->

<!-- #region colab_type="text" -->
### Explore the Keras ecosystem
<!-- #endregion -->

<!-- #region colab_type="text" -->
## Final words
<!-- #endregion -->
